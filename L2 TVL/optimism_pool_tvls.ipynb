{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python optimism_subgraph_tvls.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subgrounds.subgrounds import Subgrounds\n",
    "from subgrounds.pagination import ShallowStrategy\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import defillama_utils as dfl\n",
    "\n",
    "sgs = pd.DataFrame(\n",
    "        [\n",
    "                 ['l2dao-velodrome','https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism','']\n",
    "                ,['synthetix-curve','https://api.thegraph.com/subgraphs/name/convex-community/volume-optimism','']\n",
    "        ]\n",
    "        ,columns = ['dfl_id','subgraph_url','query']\n",
    ")\n",
    "sg = Subgrounds()\n",
    "# curve_op = sg.load_subgraph(\"https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism\")\n",
    "# display(sgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sg(tg_api):\n",
    "        csg = sg.load_subgraph(tg_api)\n",
    "        return csg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velodrome_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "        velo = create_sg('https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism')\n",
    "        q1 = velo.Query.liquidityPoolDailySnapshots(\n",
    "        orderDirection='desc',\n",
    "        first=1000,\n",
    "                where=[\n",
    "                velo.Query.liquidityPoolDailySnapshot.pool == pid,\n",
    "                velo.Query.liquidityPoolDailySnapshot.timestamp > min_ts,\n",
    "                velo.Query.liquidityPoolDailySnapshot.timestamp <= max_ts,\n",
    "                ]\n",
    "        )\n",
    "        velo_tvl = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.pool.inputTokens.id,\n",
    "                q1.pool.inputTokens.symbol,\n",
    "                \n",
    "                q1.totalValueLockedUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        velo_wts = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.inputTokenWeights,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        velo_reserves = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.inputTokenBalances,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        \n",
    "        df_array = [velo_tvl, velo_wts, velo_reserves]\n",
    "\n",
    "        for df in df_array:\n",
    "                df.columns = df.columns.str.replace('liquidityPoolDailySnapshots_', '')\n",
    "                df['id_rank'] = df.groupby(['id']).cumcount()+1\n",
    "\n",
    "        velo_tvl = velo_tvl.merge(velo_wts, on =['id','id_rank','pool_id','timestamp'])\n",
    "        velo_tvl = velo_tvl.merge(velo_reserves, on =['id','id_rank','pool_id','timestamp'])\n",
    "\n",
    "        velo_tvl['timestamp_dt'] = pd.to_datetime(velo_tvl['timestamp'],unit='s')\n",
    "        velo_tvl['timestamp_day'] = pd.to_datetime(velo_tvl['timestamp'],unit='s').dt.floor('d')\n",
    "\n",
    "        velo_tvl['inputTokenBalances'] = velo_tvl['inputTokenBalances'] / (10 ** 18)\n",
    "        velo_tvl['inputToken_tvl'] = velo_tvl['totalValueLockedUSD'] * ( velo_tvl['inputTokenWeights'] / 100 )\n",
    "        # velo_tvl['inputToken_price'] = velo_tvl['inputToken_tvl'] / velo_tvl['inputTokenBalances']\n",
    "\n",
    "        #Standardize Columns\n",
    "        # date\ttoken\ttoken_value\tusd_value\tprotocol\n",
    "        velo_tvl['protocol'] = 'Velodrome'\n",
    "        velo_tvl = velo_tvl[['timestamp_day','pool_inputTokens_symbol','inputTokenBalances','inputToken_tvl','protocol']]\n",
    "        velo_tvl = velo_tvl.rename(columns={\n",
    "                'timestamp_day':'date',\n",
    "                'pool_inputTokens_symbol':'token',\n",
    "                'inputTokenBalances':'token_value',\n",
    "                'inputToken_tvl':'usd_value'\n",
    "        })\n",
    "\n",
    "        return velo_tvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "        curve = create_sg('https://api.thegraph.com/subgraphs/name/convex-community/volume-optimism')\n",
    "        q1 = curve.Query.dailyPoolSnapshots(\n",
    "        orderDirection='desc',\n",
    "        first=1000,\n",
    "                where=[\n",
    "                curve.Query.dailyPoolSnapshot.pool == pid,\n",
    "                curve.Query.dailyPoolSnapshot.timestamp > min_ts,\n",
    "                curve.Query.dailyPoolSnapshot.timestamp <= max_ts,\n",
    "                ]\n",
    "        )\n",
    "        curve_tvl = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.pool.name,\n",
    "                q1.pool.symbol,\n",
    "                q1.timestamp,\n",
    "                # q1.tvl,\n",
    "                # q1.adminFeesUSD,\n",
    "                # q1.lpFeesUSD,\n",
    "                q1.pool.coinNames,\n",
    "                # q1.normalizedReserves,\n",
    "                # q1.reservesUSD,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        curve_reserves_normal = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.timestamp,\n",
    "                q1.normalizedReserves,\n",
    "                # q1.pool.coinNames,\n",
    "                \n",
    "                # q1.reservesUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        curve_reserves_usd = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.timestamp,\n",
    "                q1.reservesUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "\n",
    "        df_array = [curve_tvl, curve_reserves_normal, curve_reserves_usd]\n",
    "\n",
    "        for df in df_array:\n",
    "                df.columns = df.columns.str.replace('dailyPoolSnapshots_', '')\n",
    "                df['id_rank'] = df.groupby(['id']).cumcount()+1\n",
    "\n",
    "        curve_tvl = curve_tvl.merge(curve_reserves_normal, on =['id','id_rank','pool_address','timestamp'])\n",
    "        curve_tvl = curve_tvl.merge(curve_reserves_usd, on =['id','id_rank','pool_address','timestamp'])\n",
    "\n",
    "        curve_tvl['normalizedReserves'] = curve_tvl['normalizedReserves'] / ( 10 ** 18 ) #decimal adjust\n",
    "        # curve_tvl['reservePrice'] = curve_tvl['reservesUSD'] / curve_tvl['normalizedReserves'] \n",
    "        curve_tvl['timestamp_dt'] = pd.to_datetime(curve_tvl['timestamp'],unit='s')\n",
    "\n",
    "        #Standardize Columns\n",
    "        # date\ttoken\ttoken_value\tusd_value\tprotocol\n",
    "        curve_tvl['protocol'] = 'Curve'\n",
    "        curve_tvl = curve_tvl[['timestamp_dt','pool_coinNames','normalizedReserves','reservesUSD','protocol']]\n",
    "        curve_tvl = curve_tvl.rename(columns={\n",
    "                'timestamp_dt':'date',\n",
    "                'pool_coinNames':'token',\n",
    "                'normalizedReserves':'token_value',\n",
    "                'reservesUSD':'usd_value'\n",
    "        })\n",
    "\n",
    "        return curve_tvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hop_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "    api_base_str = 'https://api.llama.fi/protocol/'\n",
    "    prot_str = 'hop-protocol'\n",
    "    hop = dfl.get_single_tvl(api_base_str, prot_str, ['Optimism'])\n",
    "    hop = hop[(hop['token'] == pid) & (~hop['token_value'].isna())]\n",
    "    hop = hop[['date','token','token_value','usd_value','protocol']]\n",
    "    hop['protocol'] = 'Hop' #rename to match func\n",
    "    hop.reset_index(inplace=True,drop=True)\n",
    "    return hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this is not in TVL tracking format - maybe we split this to a new file ~eventually\n",
    "def get_messari_sg_pool_snapshots(slug, chains = ['optimism'], min_ts = 0, max_ts = 99999999999999):\n",
    "        msr_dfs = []\n",
    "        print(slug)\n",
    "        for c in chains:\n",
    "                print(c)\n",
    "                try:\n",
    "                        # Set Chain\n",
    "                        curve = create_sg('https://api.thegraph.com/subgraphs/name/messari/' + slug + '-' + c)\n",
    "                        # Get Query\n",
    "                        q1 = curve.Query.liquidityPoolDailySnapshots(\n",
    "                        orderDirection='desc',\n",
    "                        first=1000,\n",
    "                                where=[\n",
    "                                curve.Query.liquidityPoolDailySnapshot.timestamp > min_ts,\n",
    "                                curve.Query.liquidityPoolDailySnapshot.timestamp <= max_ts,\n",
    "                                ]\n",
    "                        )\n",
    "                        # Pull Fields\n",
    "                        msr_list = sg.query_df([\n",
    "                                q1.id,\n",
    "                                q1.timestamp,\n",
    "                                q1.totalValueLockedUSD,\n",
    "                                q1.dailyVolumeUSD,\n",
    "                                q1.rewardTokenEmissionsUSD,\n",
    "                                #protocol\n",
    "                                q1.protocol.id,\n",
    "                                q1.protocol.name,\n",
    "                                q1.protocol.slug,\n",
    "                                q1.protocol.network,\n",
    "                                #pool\n",
    "                                q1.pool.id,\n",
    "                                q1.pool.name,\n",
    "                                q1.pool.symbol,\n",
    "                                q1.pool.inputTokens.id\n",
    "                        ]\n",
    "                        , pagination_strategy=ShallowStrategy)\n",
    "                        msr_df = pd.concat(msr_list)\n",
    "                except:\n",
    "                        msr_df = pd.DataFrame()\n",
    "                        continue\n",
    "                msr_dfs.append(msr_df)\n",
    "        \n",
    "        #combine all chains\n",
    "        msr_daily = pd.concat(msr_dfs)\n",
    "\n",
    "        #fix up column names\n",
    "\n",
    "        msr_daily.columns = msr_daily.columns.str.replace('liquidityPoolDailySnapshots_', '')\n",
    "\n",
    "        col_list = msr_daily.columns.to_list()\n",
    "        col_list.remove('pool_inputTokens_id') # we want to group by everything else \n",
    "\n",
    "        print(col_list)\n",
    "        \n",
    "        msr_daily = msr_daily.fillna(0)\n",
    "\n",
    "        msr_daily = msr_daily.groupby(col_list).agg({'pool_inputTokens_id':lambda x: list(x)})\n",
    "        msr_daily.reset_index(inplace=True)\n",
    "\n",
    "        msr_daily['timestamp'] = pd.to_datetime(msr_daily['timestamp'],unit='s')\n",
    "        msr_daily['date'] = msr_daily['timestamp'].dt.floor('d')\n",
    "        msr_daily['id_rank'] = msr_daily.groupby(['id']).cumcount()+1\n",
    "        \n",
    "        return pd.DataFrame(msr_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = get_curve_pool_tvl('0x061b87122ed14b9526a813209c8a59a633257bab')\n",
    "# vdf = get_velodrome_pool_tvl('0xfc77e39de40e54f820e313039207dc850e4c9e60')\n",
    "# get_hop_pool_tvl('SNX')\n",
    "# display(vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.\n",
    "\n",
    "# msr = get_messari_sg_pool_snapshots('curve-finance')\n",
    "# display(msr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python optimism_pool_tvls.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d537a1638226190f579d6fbb68604c1b09ebc740a69df557abedb49ad78e592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
