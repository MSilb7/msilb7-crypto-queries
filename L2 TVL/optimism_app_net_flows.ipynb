{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install requests\n",
    "# ! pip install plotly\n",
    "# ! pip install datetime\n",
    "# ! pip install os\n",
    "# ! pip freeze = requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as r\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import optimism_pool_tvls as subg\n",
    "import defillama_utils as dfl\n",
    "import pandas_utils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "if 'L2 TVL' in pwd:\n",
    "    prepend = ''\n",
    "else:\n",
    "    prepend = 'L2 TVL/'\n",
    "\n",
    "# if TVL by token is not available, do we fallback on raw TVL (sensitive to token prices)?\n",
    "do_fallback_on_raw_tvl = True\n",
    "str_fallback_indicator = '' #Dont append any indicator yet, since it screws up joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol Incentive Start Dates\n",
    "# Eventually, move this to its own file / csv\n",
    "protocols = pd.DataFrame(\n",
    "    [\n",
    "        # ['include_in_summary','protocol','num_op','start_date', 'end_date','name', 'op_source', 'data_source','contracts']\n",
    "            # General Programs\n",
    "             [1,'velodrome',  3000000,          '2022-07-13',   '2022-11-17',   'Velodrome #1 ', 'Partner Fund', 'defillama','','']\n",
    "            ,[1,'pooltogether',   450000, '2022-07-22',   '', '', 'Partner Fund', 'defillama','','']\n",
    "            ,[1,'lyra',   3000000,               '2022-08-02',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'rubicon',    900000,            '2022-07-15',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'perpetual-protocol', 9000000, '2022-07-14',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'thales', 900000,             '2022-07-15',   '',   '', 'Gov Fund - Phase 0', 'defillama','',''] #TVL not relevant\n",
    "            ,[1,'aave-v3',    5000000,            '2022-08-04',   '2022-11-04',   'Aave - Liquidity Mining', 'Partner Fund', 'defillama','','']\n",
    "            ,[1,'wepiggy',    300000,            '2022-08-03',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'stargate',   1000000,           '2022-08-05',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'pika-protocol',  900000,      '2022-08-29',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'pickle', 200000,             '2022-09-09',   '',   'Pickle Finance', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'aelin',  900000,              '2022-09-12',   '2022-09-14',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'polynomial-protocol',    900000, '2022-09-14',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'xtoken', 900000,             '2022-09-19',   '',   '', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'hop-protocol',   1000000,       '2022-09-22',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'beethoven-x',    500000,        '2022-09-29',   '',   '', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'beefy',  650000*0.5,              '2022-10-24',   '',   '', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'hundred-finance',    300000,    '2022-11-28',   '',   '', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'dforce',    300000,    '2022-11-30',   '',   '', 'Gov Fund - Season 1', 'defillama','','']\n",
    "            ,[1,'cbridge',    300000,    '2022-08-13',   '',   'Celer', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'clipper',    300000,    '2023-01-23',   '',   '', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            #Uniswap LM Program - Phase 1\n",
    "            ,[0,'uniswap-v3', 150000,         '2022-10-26',   '2022-11-21',   'Uniswap LM - Phase 1', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'arrakis-finance',    50000,    '2022-10-26',   '2022-11-21',   'Uniswap LM - Phase 1', 'Gov Fund - Phase 0','defillama','','']\n",
    "            ,[1,'gamma',    50000,              '2022-10-26',   '2022-11-21',   'Uniswap LM - Phase 1', 'Gov Fund - Phase 0','defillama','','']\n",
    "            ,[1,'xtoken',    50000,             '2022-10-26',   '2022-11-21',   'Uniswap LM - Phase 1', 'Gov Fund - Phase 0','defillama','','']\n",
    "            #Uniswap LM Program - Phase 2 - https://gov.uniswap.org/t/rfc-phase-2-optimism-uniswap-protocol-liquidity-mining-program/19803/12\n",
    "            ,[0,'uniswap-v3', 100000,         '2023-01-16',   '2023-02-08',   'Uniswap LM - Phase 2', 'Gov Fund - Phase 0', 'defillama','','']\n",
    "            ,[1,'arrakis-finance',    100000/4,    '2023-01-16',   '2023-02-08',   'Uniswap LM - Phase 2', 'Gov Fund - Phase 0','defillama','','']\n",
    "            ,[1,'gamma',    100000/4,              '2023-01-16',   '2023-02-08',   'Uniswap LM - Phase 2', 'Gov Fund - Phase 0','defillama','','']\n",
    "            ,[1,'xtoken',    100000/4,             '2023-01-16',   '2023-02-08',   'Uniswap LM - Phase 2', 'Gov Fund - Phase 0','defillama','','']\n",
    "            ,[1,'defiedge',    100000/4,             '2023-01-16',   '2023-02-08',   'Uniswap LM - Phase 2', 'Gov Fund - Phase 0','defillama','','']\n",
    "            # Other DEX Programs\n",
    "            \n",
    "            ,[1,'qidao',  7_000*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-08-26')).days / 7 ),    '2022-08-26',   '', 'USDC/MAI: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0xd62c9d8a3d4fd98b27caaefe3571782a3af0a737'],'velodrome']\n",
    "            ,[1,'dhedge',  1_900*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2023-02-03')).days / 7 ),    '2023-02-03',   '', 'DHT/OP: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0x827ecd158b76f63010e8f129b19fe64a85e97e95'],'velodrome']\n",
    "            ,[1,'rocket-pool',  2_100*(abs(pd.to_datetime('2023-02-08')-pd.to_datetime('2022-11-09')).days / 7 ),    '2022-11-09',   '2023-02-08', 'sWETH/rETH: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0x69f795e2d9249021798645d784229e5bec2a5a25'],'velodrome']\n",
    "            ,[1,'rocket-pool',  4_300*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2023-02-08')).days / 7 ),    '2023-02-08',   '', 'vWETH/rETH: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0x985612ff2c9409174fedcff23d4f4761af124f88'],'velodrome']\n",
    "            ,[1,'rocket-pool',  5_000*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-11-09')).days / 7 ),    '2022-11-09',   '', 'WETH/rETH: Beethoven X', 'Gov Fund - Season 1',  'pool-subgraph-messari',  ['0x4fd63966879300cafafbb35d157dc5229278ed23'],'beethoven-x']\n",
    "            ,[0,'synthetix',  2*20000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-08-25')).days / 7 ),    '2022-08-25',   '',   'All Synthetix Curve Pools', 'Gov Fund - Phase 0', 'pool-subgraph-curve',['0x7bc5728bc2b59b45a58d9a576e2ffc5f0505b35e','0x061b87122ed14b9526a813209c8a59a633257bab'],'curve'] # susd/usd + seth/eth Curve incentives started\n",
    "            ,[1,'synthetix',  20000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-08-25')).days / 7 ),    '2022-08-25',   '',   'sUSD-3Crv: Curve', 'Gov Fund - Phase 0', 'pool-subgraph-curve',['0x061b87122ed14b9526a813209c8a59a633257bab'],'curve'] # susd/usd + seth/eth Curve incentives started\n",
    "            ,[1,'synthetix',  20000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-08-25')).days / 7 ),    '2022-08-25',   '',   'sETH-ETH: Curve', 'Gov Fund - Phase 0', 'pool-subgraph-curve',['0x7bc5728bc2b59b45a58d9a576e2ffc5f0505b35e'],'curve'] # susd/usd + seth/eth Curve incentives started\n",
    "            #More DEXs\n",
    "    \n",
    "            ,[1,'overnight',  3_000*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2023-01-10')).days / 7 ),    '2023-01-10',   '', 'USD+/USDC: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0x67124355cce2ad7a8ea283e990612ebe12730175'],'velodrome']\n",
    "            ,[1,'overnight',  2_500,    '2023-01-10',   '', 'BPT-USD+: Beethoven X', 'Gov Fund - Season 1',  'pool-subgraph-messari',  ['0xb1c9ac57594e9b1ec0f3787d9f6744ef4cb0a024'],'beethoven-x']\n",
    "            ,[1,'overnight',  1_000*(abs(pd.to_datetime(\"today\")-pd.to_datetime('2023-01-30')).days / 7 ),    '2023-01-30',   '', 'USD+/DOLA: Velodrome', 'Gov Fund - Season 1',  'pool-subgraph-velodrome',  ['0xa99817d2d286c894f8f3888096a5616d06f20d46'],'velodrome']\n",
    "            \n",
    "            ,[0,'synthetix',  3*10000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-09-27')).days / 7 ),    '2022-09-27',   '',   'All Synthetix Velo Pools', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0x9056eb7ca982a5dd65a584189994e6a27318067d' \\\n",
    "                                                                                                                                                                                                    ,'0xd16232ad60188b68076a235c65d692090caba155'\\\n",
    "                                                                                                                                                                                                    ,'0xfd7fddfc0a729ecf45fb6b12fa3b71a575e1966f'],'velodrome'] # Velo incentives started\n",
    "            ,[1,'synthetix',  10000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-09-27')).days / 7 ),    '2022-09-27',   '',   'USDC/SNX: Velo', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0x9056eb7ca982a5dd65a584189994e6a27318067d'],'velodrome'] # Velo incentives started\n",
    "            ,[1,'synthetix',  10000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-09-27')).days / 7 ),    '2022-09-27',   '',   'USDC/sUSD: Velo', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0xd16232ad60188b68076a235c65d692090caba155'],'velodrome'] # Velo incentives started\n",
    "            ,[1,'synthetix',  10000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-09-27')).days / 7 ),    '2022-09-27',   '',   'ETH/sETH: Velo', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0xfd7fddfc0a729ecf45fb6b12fa3b71a575e1966f'],'velodrome'] # Velo incentives started\n",
    "\n",
    "            ,[1,'synthetix',  18000* (abs(pd.to_datetime(\"today\")-pd.to_datetime('2022-10-25')).days / (7*4) ),    '2022-10-25',   '',   'SNX Bridge: Hop', 'Gov Fund - Phase 0', 'pool-defillama-hop',['SNX'],'hop'] # Hop incentives started\n",
    "            \n",
    "            ,[1,'layer2dao',  300000,    '2022-07-20',   '2022-08-22',   'L2DAO/OP: Velodrome', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0xfc77e39de40e54f820e313039207dc850e4c9e60'],'velodrome'] # l2dao/op incentives - estimating end date based on last distribution to Velo gauge + 7 days\n",
    "            ,[1,'beefy',  650000*0.35,    '2022-09-13',   '',   'BIFI/OP: Velodrome', 'Gov Fund - Phase 0', 'pool-subgraph-velodrome',['0x81f638e5d063618fc5f6a976e48e9b803b3240c0'],'velodrome'] # bifi/op incentives\n",
    "            # Season 2\n",
    "            ,[1,'velodrome',  4000000,  '2022-11-24',   '',   'Velodrome #2 (Tour de OP)', 'Gov Fund - Season 2', 'defillama','','']\n",
    "            ,[1,'revert-compoundor',  240000,  '2022-11-03',   '',   '', 'Gov Fund - Season 2', 'defillama','','']\n",
    "            ,[1,'dhedge',    350000,    '2022-12-21',   '',   '', 'Gov Fund - Season 2', 'defillama','',''] # Announced 12/21, launched 1/17 - https://twitter.com/dHedgeOrg/status/1615573828394184706\n",
    "            ]\n",
    "        , columns = ['include_in_summary','protocol','num_op','start_date', 'end_date','name', 'op_source', 'data_source','contracts','source_slug']\n",
    "    )\n",
    "\n",
    "protocol_name_mapping = pd.DataFrame(\n",
    "    [\n",
    "        ['aave-v3','aave'],\n",
    "        ['beefy','beefy finance'],\n",
    "        ['revert-compoundor','revert finance'],\n",
    "        ['cbridge','celer'],\n",
    "        ['pickle','pickle finance'],\n",
    "        ['stargate','stargate finance']\n",
    "    ]\n",
    "    ,columns=['protocol','app_name']\n",
    ")\n",
    "\n",
    "protocols = protocols.merge(protocol_name_mapping,on='protocol', how = 'left')\n",
    "protocols['app_name'] = protocols['app_name'].combine_first(protocols['protocol'])\n",
    "\n",
    "# print(protocols[0])\n",
    "protocols['id_format'] = protocols['protocol'].str.replace('-',' ').str.title()\n",
    "\n",
    "protocols['app_name'] = protocols['app_name'].str.replace('-',' ').str.title()\n",
    "\n",
    "date_cols = ['start_date', 'end_date']\n",
    "for d in date_cols:\n",
    "    protocols[d] = pd.to_datetime( protocols[d] )\n",
    "    \n",
    "\n",
    "# protocols['program_name'] = np.where( protocols['name'] == '', protocols['id_format'], protocols['name'])\n",
    "protocols['top_level_name'] = np.where( protocols['name'] == ''\n",
    "                                    , protocols['id_format']\n",
    "                                    , protocols['name']\n",
    "                                    )\n",
    "# Get count by coalesced name\n",
    "pcounts = pd.DataFrame( protocols.groupby(['top_level_name'])['name'].count() )\n",
    "pcounts = pcounts.rename(columns={'name':'count'})\n",
    "\n",
    "protocols = protocols.merge(pcounts, on = 'top_level_name')\n",
    "\n",
    "protocols['slug'] = protocols['protocol']\n",
    "protocols['program_name'] = np.where( ( (protocols['name'] == '') )#| (protocols['count'] == 1) )\n",
    "                                    , protocols['id_format']\n",
    "                                    , protocols['id_format'] + ' - ' + protocols['name']\n",
    "                                    )\n",
    "\n",
    "protocols = protocols.sort_values(by='start_date', ascending=True)\n",
    "                    \n",
    "# display(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3399236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl_protocols = protocols[protocols['data_source'] == 'defillama'].copy()\n",
    "\n",
    "dfl_slugs = dfl_protocols[['protocol']].drop_duplicates()\n",
    "# display(dfl_slugs)\n",
    "dfl_slugs = dfl_slugs.rename(columns={'protocol':'slug'})\n",
    "df_dfl = dfl.get_range(dfl_slugs[['slug']],['Optimism'], fallback_on_raw_tvl= do_fallback_on_raw_tvl)\n",
    "# display(df_dfl[df_dfl['protocol']=='beefy'])\n",
    "df_dfl = df_dfl.merge(dfl_protocols, on ='slug')\n",
    "\n",
    "df_dfl['protocol'] = df_dfl['protocol_y'].combine_first(df_dfl['protocol_x'])\n",
    "df_dfl['name'] = df_dfl['name_y'].combine_first(df_dfl['name_x'])\n",
    "\n",
    "df_dfl = df_dfl[['date', 'token', 'token_value', 'usd_value', 'protocol', 'start_date','end_date','program_name','app_name','top_level_name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d797655",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg_protocols = protocols[protocols['data_source'].str.contains('pool-')].copy()\n",
    "# subg_protocols['og_app_name'] = subg_protocols['app_name']\n",
    "subg_protocols['og_protocol'] = subg_protocols['protocol']\n",
    "subg_protocols['df_source'] = subg_protocols['data_source'].str.split('-').str[-1]\n",
    "# display(subg_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs_sub = []\n",
    "for index, program in subg_protocols.iterrows():\n",
    "        min_tsmp = int( pd.to_datetime(program['start_date']).timestamp() )\n",
    "        min_tsmp = min_tsmp - 1000 #add some buffer\n",
    "        source_slug = program['source_slug']\n",
    "        df_source = program['df_source']\n",
    "        print(df_source + ' - ' +source_slug)\n",
    "        for c in program['contracts']:\n",
    "                # messari generalized\n",
    "                if df_source == 'messari':\n",
    "                        sdf = subg.get_messari_format_pool_tvl(source_slug, c, min_ts = min_tsmp)\n",
    "                # subgraph specific\n",
    "                elif df_source == 'curve':\n",
    "                        sdf = subg.get_curve_pool_tvl(c, min_ts = min_tsmp)\n",
    "                elif df_source == 'velodrome':\n",
    "                        sdf = subg.get_velodrome_pool_tvl(c, min_ts = min_tsmp)\n",
    "                elif df_source == 'hop':\n",
    "                        sdf = subg.get_hop_pool_tvl(c, min_ts = min_tsmp)\n",
    "                        \n",
    "                sdf['start_date'] = program['start_date']\n",
    "                sdf['end_date'] = program['end_date']\n",
    "                sdf['program_name'] = program['program_name']\n",
    "                sdf['protocol'] = program['og_protocol']\n",
    "                sdf['app_name'] = program['app_name']\n",
    "                sdf['top_level_name'] = program['top_level_name']\n",
    "\n",
    "                sdf['token_value'] = sdf['token_value'].fillna(0)\n",
    "                sdf['usd_value'] = sdf['usd_value'].fillna(0)\n",
    "                dfs_sub.append(sdf)\n",
    "df_df_sub = pd.concat(dfs_sub)\n",
    "# display(df_df_sub[df_df_sub['program_name'].str.contains('Velo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21088526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_df_sub.sort_values(by='date'))\n",
    "# display(df_dfl[df_dfl['protocol']=='defiedge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df_comb = pd.concat([df_dfl, df_df_sub])\n",
    "# display(df_df_comb)\n",
    "df_df_comb['start_date'] = pd.to_datetime(df_df_comb['start_date'])\n",
    "df_df_comb['end_date'] = pd.to_datetime(df_df_comb['end_date'])\n",
    "df_df_comb['date'] = pd.to_datetime(df_df_comb['date'])\n",
    "# display(df_df_comb)\n",
    "\n",
    "# Make sure datatypes are clean\n",
    "df_df_comb['token_value'] = df_df_comb['token_value'].astype('float64')\n",
    "df_df_comb['usd_value'] = df_df_comb['usd_value'].astype('float64')\n",
    "\n",
    "#create an extra day to handle for tokens dropping to 0\n",
    "#this is a temp fix - longer term also: Get max of a token x date and do date + 1 = 0 (i.e. weth to eth flips)\n",
    "# find intermediate gaps. Call it a 0 flow in the in-between dates (i.e. pooltogether)\n",
    "df_df_shift = df_df_comb.copy()\n",
    "df_df_shift['date'] = df_df_shift['date'] + timedelta(days=1)\n",
    "df_df_shift['token_value'] = 0\n",
    "df_df_shift['usd_value'] = 0\n",
    "\n",
    "#merge back in\n",
    "df_df = pd.concat([df_df_comb,df_df_shift])\n",
    "df_df = df_df[df_df['date'] <= pd.to_datetime(\"today\") ]\n",
    "\n",
    "# Group - Exclude End Date since this is often null and overwritting could be weird, especially if we actually know an end date\n",
    "df_df['start_date'] = df_df['start_date'].fillna( pd.to_datetime(\"today\").floor('d') )\n",
    "#Generate End Date Column\n",
    "df_df['end_date_30'] = df_df['end_date'].fillna(pd.to_datetime(\"today\")).dt.floor('d') + timedelta(days = 30)\n",
    "\n",
    "df_df = df_df.groupby(['date','token','protocol','start_date','end_date_30','program_name','app_name','top_level_name']).sum(numeric_only=True).reset_index()\n",
    "\n",
    "# display(\n",
    "#         df_df[(df_df['protocol']=='revert-compoundor') & (df_df['date'] == '2022-11-09')] \n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9eb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_df.copy()#merge(cg_df, on=['date','token'],how='inner')\n",
    "\n",
    "# data_df = data_df[data_df['token_value'] > 0] #Exclude this, so we can read flows\n",
    "\n",
    "data_df.sort_values(by='date',inplace=True)\n",
    "# data_df['token_value'] = data_df['token_value'].replace(0, np.nan) #keep zeroes\n",
    "data_df['price_usd'] = data_df['usd_value']/data_df['token_value']\n",
    "\n",
    "data_df['rank_desc'] = data_df.groupby(['protocol', 'program_name', 'token'])['date'].\\\n",
    "                            rank(method='dense',ascending=False).astype(int)\n",
    "\n",
    "data_df.sort_values(by='date',inplace=True)\n",
    "\n",
    "last_df = data_df[data_df['rank_desc'] == 1]\n",
    "last_df = last_df.rename(columns={'price_usd':'last_price_usd'})\n",
    "last_df = last_df[['token','protocol','program_name','last_price_usd']]\n",
    "# display(last_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(last_df, on=['token','protocol','program_name'], how='left')\n",
    "\n",
    "data_df['last_token_value'] = data_df.groupby(['token','protocol', 'program_name'])['token_value'].shift(1)\n",
    "\n",
    "data_df['last_price_usd'] = data_df.groupby(['token','protocol', 'program_name'])['price_usd'].shift(1)\n",
    "\n",
    "# If first instnace of token, make sure there's no price diff\n",
    "data_df['last_price_usd'] = data_df[['last_price_usd', 'price_usd']].bfill(axis=1).iloc[:, 0]\n",
    "#Forward fill if token drops off\n",
    "data_df['price_usd'] = data_df[['price_usd','last_price_usd']].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "data_df['last_token_value'] = data_df['last_token_value'].fillna(0)\n",
    "\n",
    "data_df['net_token_flow'] = data_df['token_value'] - data_df['last_token_value']\n",
    "data_df['net_price_change'] = data_df['price_usd'] - data_df['last_price_usd']\n",
    "\n",
    "data_df['net_dollar_flow'] = data_df['net_token_flow'] * data_df['price_usd']\n",
    "data_df['last_price_net_dollar_flow'] = data_df['net_token_flow'] * data_df['last_price_usd']\n",
    "\n",
    "data_df['net_price_stock_change'] = data_df['last_token_value'] * data_df['net_price_change']\n",
    "\n",
    "\n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e56fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter before start date\n",
    "data_df = data_df[data_df['date']>= data_df['start_date']]\n",
    "# filter lte end date + 30\n",
    "data_df = data_df[data_df['date']<= data_df['end_date_30']]\n",
    "data_df.drop('end_date_30', axis=1, inplace=True)\n",
    "\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "data_df.to_csv(prepend + 'csv_outputs/' + 'tvl_flows_by_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[data_df['protocol']=='perpetual-protocol'].sort_values(by='date')\n",
    "# data_df.fillna(0)\n",
    "# data_df.sample(5)\n",
    "# data_df[(data_df['protocol'] == 'pooltogether') & (data_df['date'] >= '2022-10-06') & (data_df['date'] <= '2022-10-12')].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "netdf_df = data_df[['date','protocol','program_name','net_dollar_flow','net_price_stock_change','last_price_net_dollar_flow','usd_value','app_name','top_level_name']]\n",
    "\n",
    "netdf_df = netdf_df.groupby(['date','protocol','program_name','app_name','top_level_name']).sum(['net_dollar_flow','net_price_stock_change','last_price_net_dollar_flow','usd_value'])\n",
    "\n",
    "# reset & get program data\n",
    "netdf_df.reset_index(inplace=True)\n",
    "\n",
    "netdf_df['tvl_change'] = netdf_df['usd_value'] - netdf_df.groupby(['protocol', 'program_name','app_name'])['usd_value'].shift(1)\n",
    "netdf_df['error'] = netdf_df['tvl_change'] - (netdf_df['net_dollar_flow'] + netdf_df['net_price_stock_change'])\n",
    "\n",
    "cumul_cols = ['net_dollar_flow','last_price_net_dollar_flow','net_price_stock_change']\n",
    "for c in cumul_cols:\n",
    "        netdf_df['cumul_' + c] = netdf_df.groupby(['protocol', 'program_name'])[c].cumsum()\n",
    "        # netdf_df['cumul_last_price_net_dollar_flow'] = netdf_df.groupby(['protocol', 'program_name'])['last_price_net_dollar_flow'].cumsum()\n",
    "        # netdf_df['cumul_net_price_stock_change'] = netdf_df.groupby(['protocol', 'program_name'])['net_price_stock_change'].cumsum()\n",
    "\n",
    "\n",
    "# print(protocols.columns)\n",
    "# print(netdf_df.columns)\n",
    "\n",
    "# Bring Program info Back In\n",
    "netdf_df = netdf_df.merge(protocols[['include_in_summary','program_name','app_name','top_level_name','protocol','op_source','start_date','end_date','num_op']], on=['program_name','protocol','app_name','top_level_name'])\n",
    "\n",
    "#For Summary\n",
    "if_ended_cols = ['net_dollar_flow','last_price_net_dollar_flow']\n",
    "new_ended_cols = []\n",
    "for e in if_ended_cols:\n",
    "        netdf_df['cumul_' + e + '_if_ended'] = netdf_df[~netdf_df['end_date'].isna()].groupby(['protocol', 'program_name'])[e].cumsum()\n",
    "        new_ended_cols.append('cumul_' + e + '_if_ended')\n",
    "#\n",
    "# print(new_ended_cols)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'revert-compoundor'])\n",
    "\n",
    "for d in date_cols:\n",
    "        netdf_df[d] = pd.to_datetime(netdf_df[d])\n",
    "\n",
    "# check info at program end\n",
    "# display(program_end_df)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'dhedge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = ['cumul_net_dollar_flow','cumul_last_price_net_dollar_flow','cumul_net_price_stock_change','num_op']\n",
    "\n",
    "netdf_df['program_rank_desc'] = netdf_df.groupby(['protocol', 'program_name'])['date'].\\\n",
    "                            rank(method='dense',ascending=False).astype(int)\n",
    "\n",
    "# for sc in summary_cols:\n",
    "#         netdf_df[sc] = netdf_df[sc].astype('int64')\n",
    "summary_cols = summary_cols + new_ended_cols\n",
    "# print(summary_cols)\n",
    "program_end_df = netdf_df[\n",
    "        (pd.to_datetime(netdf_df['date']) == pd.to_datetime(netdf_df['end_date']) ) # is at end date\n",
    "        | (netdf_df['program_rank_desc'] ==1)# or is latest date\n",
    "                        ].groupby(['protocol', 'program_name','app_name']).sum(numeric_only=True)\n",
    "program_end_df.reset_index(inplace=True)\n",
    "# display(program_end_df)\n",
    "\n",
    "# display(program_end_df)\n",
    "for s in summary_cols:\n",
    "        s_new = s+'_at_program_end'\n",
    "        program_end_df = program_end_df.rename(columns={s:s_new})\n",
    "        netdf_df = netdf_df.merge(program_end_df[['protocol','program_name',s_new]], on=['protocol','program_name'], how = 'left')\n",
    "\n",
    "# netdf_df['cumul_net_dollar_flow_at_program_end'] = netdf_df[is_program_end].groupby(['protocol', 'program_name']).sum(['cumul_net_dollar_flow'])\n",
    "# netdf_df['cumul_last_price_net_dollar_flow_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['last_price_net_dollar_flow'].groupby(['protocol', 'program_name']).cumsum()\n",
    "# netdf_df['cumul_net_price_stock_change_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['net_price_stock_change'].groupby(['protocol', 'program_name']).cumsum()\n",
    "\n",
    "# netdf_df.loc[ netdf_df['end_date'] == pd.to_datetime(\"2000-01-01\"), 'end_date' ] == pd.to_datetime(\"1900-01-01\")\n",
    "\n",
    "# np.where( netdf_df['end_date'] <= pd.to_datetime(\"2000-01-01\") , pd.NaT , netdf_df['end_date'] )\n",
    "# display(netdf_df[netdf_df['protocol'] == 'hundred-finance'].sort_values(by='program_rank_desc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb142e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netdf_df[(netdf_df['date'] >= '2022-10-06') & (netdf_df['date'] <= '2022-10-12')].tail(10)\n",
    "# netdf_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "during_str = 'During Program'\n",
    "post_str = 'Post-Program'\n",
    "\n",
    "netdf_df['period'] = np.where(\n",
    "        netdf_df['date'] > netdf_df['end_date'], post_str, during_str\n",
    "        )\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "netdf_df.to_csv(prepend + 'csv_outputs/op_summer_daily_stats.csv', index=False)\n",
    "\n",
    "#SORT FOR CHARTS\n",
    "netdf_df = netdf_df.sort_values(by=['top_level_name','program_name','app_name'], ascending=[True,True,True])\n",
    "# display(netdf_df.head())\n",
    "# print(netdf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data_df = netdf_df[netdf_df['program_rank_desc'] == 1]\n",
    "latest_data_df['date'] = latest_data_df['date'].dt.date\n",
    "# latest_data_df['days_since_program_end'] \n",
    "# latest_data_df.loc[latest_data_df['end_date'] != '', 'days_since_program_end'] = \\\n",
    "#         pd.to_datetime(latest_data_df['end_date']) \\\n",
    "#         - pd.to_datetime(latest_data_df['date'])\n",
    "\n",
    "latest_data_df['days_since_program_end'] = \\\n",
    "        np.where(latest_data_df['end_date'] != '',\n",
    "        pd.to_datetime(latest_data_df['end_date']) \\\n",
    "        - pd.to_datetime(latest_data_df['date']) \\\n",
    "        , \\\n",
    "        pd.to_datetime(latest_data_df['date']) \\\n",
    "        - pd.to_datetime(latest_data_df['start_date']) \\\n",
    "        )\n",
    "latest_data_df = latest_data_df.sort_values(by='start_date', ascending=False)\n",
    "# display(latest_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate agg summary df\n",
    "season_summary_pds = latest_data_df[latest_data_df['include_in_summary'] == 1].copy()\n",
    "\n",
    "season_summary_s0_no_perp = season_summary_pds[(season_summary_pds['op_source'] == 'Gov Fund - Phase 0') \\\n",
    "                                                & (season_summary_pds['protocol'] != 'perpetual-protocol')]\n",
    "\n",
    "season_summary_s0_no_perp['op_source'] = 'Gov Fund - Phase 0 (Excl. Perp)'\n",
    "\n",
    "season_summary_raw = pd.concat([season_summary_pds, season_summary_s0_no_perp])\n",
    "\n",
    "season_summary_completed_raw = season_summary_pds[season_summary_pds['end_date'] < pd.to_datetime(\"today\")] #only ended summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEASON SUMMARY\n",
    "season_summary = season_summary_raw.groupby('op_source').sum()\n",
    "# display(season_summary.head())\n",
    "season_summary.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_total_raw = season_summary_raw.copy()\n",
    "season_summary_total_raw['op_source'] = '- TOTAL -'\n",
    "season_summary_total = pd.DataFrame(season_summary_total_raw.groupby('op_source').sum())\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary = pd.concat([season_summary, season_summary_total])\n",
    "season_summary.reset_index(inplace=True)\n",
    "# season_summary.head()\n",
    "\n",
    "# SEASON SUMMARY IF COMPLETED - loops were weird, so doing it this way\n",
    "\n",
    "season_summary_completed = season_summary_completed_raw.groupby('op_source').sum()\n",
    "# display(season_summary.head())\n",
    "season_summary_completed.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_completed_total_raw = season_summary_completed_raw.copy()\n",
    "season_summary_completed_total_raw['op_source'] = '- TOTAL -'\n",
    "season_summary_completed_total = pd.DataFrame(season_summary_completed_total_raw.groupby('op_source').sum())\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary_completed = pd.concat([season_summary_completed, season_summary_completed_total])\n",
    "season_summary_completed.reset_index(inplace=True)\n",
    "# season_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(latest_data_df.columns)\n",
    "# print(season_summary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [latest_data_df, season_summary, season_summary_completed]\n",
    "latest_data_df.name = 'op_summer_latest'\n",
    "season_summary.name = 'season_summary'\n",
    "season_summary_completed.name = 'season_summary_completed'\n",
    "\n",
    "for df in df_list:\n",
    "        # Fix 0 columns\n",
    "        for col in df.columns:\n",
    "                if \"_at_program_end\" in col:\n",
    "                        df[col] = df[col].astype(float)\n",
    "                        df[col] = np.where(df[col] == 0, np.NaN, df[col])\n",
    "\n",
    "        df['cumul_flows_per_op_at_program_end'] = df['cumul_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "        \n",
    "        df['cumul_flows_per_op_latest'] = df['cumul_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "        df['last_price_net_dollar_flows_per_op_at_program_end'] = df['cumul_last_price_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "        df['last_price_net_dollar_flows_per_op_latest'] = df['cumul_last_price_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "        df['flows_retention'] = \\\n",
    "                        df['cumul_net_dollar_flow_if_ended'] / df['cumul_net_dollar_flow_at_program_end'] \\\n",
    "                        * np.where(df['cumul_net_dollar_flow'] < 0, -1, 1)\n",
    "        df['last_price_net_dollar_flows_retention'] = \\\n",
    "                        df['cumul_last_price_net_dollar_flow_if_ended'] / df['cumul_last_price_net_dollar_flow_at_program_end'] \\\n",
    "                        * np.where(df['cumul_last_price_net_dollar_flow'] < 0, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74469ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # display(df)\n",
    "    #get df name\n",
    "    col_list = [\n",
    "        'date','include_in_summary','program_name','app_name','num_op','period','op_source','start_date','end_date'\n",
    "        ,'cumul_net_dollar_flow_at_program_end'\n",
    "        ,'cumul_net_dollar_flow'\n",
    "        ,'cumul_flows_per_op_at_program_end','cumul_last_price_net_dollar_flow_at_program_end'\n",
    "        ,'cumul_flows_per_op_latest', 'cumul_last_price_net_dollar_flow'\n",
    "        , 'last_price_net_dollar_flows_per_op_at_program_end','last_price_net_dollar_flows_per_op_latest'\n",
    "        ,'flows_retention', 'last_price_net_dollar_flows_retention'\n",
    "    ]\n",
    "    summary_exclude_list = ['date','program_name','app_name','period','start_date','end_date']\n",
    "    sort_cols = ['Start','# OP']\n",
    "\n",
    "    if df.name == 'op_summer_latest':\n",
    "        html_name = df.name + '_stats'\n",
    "        sort_order = [False, False]\n",
    "    elif 'season_summary' in df.name:\n",
    "        html_name = df.name + '_stats'\n",
    "        sort_cols = ['Source','# OP']\n",
    "        sort_order = [False, True] # so totals goes to bottom\n",
    "        col_list = [x for x in col_list if x not in summary_exclude_list]\n",
    "    else:\n",
    "        html_name = 'other'\n",
    "\n",
    "    df_format = df.copy()\n",
    "    new_cols = df_format.columns\n",
    "    drop_cols = ['net_dollar_flow',\n",
    "        'net_price_stock_change', 'last_price_net_dollar_flow', 'usd_value',\n",
    "        'tvl_change', 'error'\n",
    "        ]\n",
    "    new_cols = new_cols.drop(drop_cols)\n",
    "    # print(new_cols)\n",
    "    df_format = df_format[new_cols]\n",
    "\n",
    "    # df_format['num_op'] = df_format['num_op'].apply(lambda x: '{0:,.0f}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['flows_retention'] = df_format['flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['last_price_net_dollar_flows_retention'] = df_format['last_price_net_dollar_flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "\n",
    "    df_format = df_format[col_list]\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "\n",
    "    if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "    df_format.to_csv(prepend + 'csv_outputs/' + html_name + '.csv', index=False)\n",
    "\n",
    "    format_cols = [\n",
    "        'cumul_flows_per_op_at_program_end','cumul_flows_per_op_latest','last_price_net_dollar_flows_per_op_at_program_end','last_price_net_dollar_flows_per_op_latest']\n",
    "    format_mil_cols = [\n",
    "        'cumul_net_dollar_flow', 'cumul_last_price_net_dollar_flow',\n",
    "        'cumul_net_dollar_flow_at_program_end',\n",
    "        'cumul_last_price_net_dollar_flow_at_program_end'\n",
    "    ]\n",
    "    # for f in format_cols:\n",
    "        # df_format[f] = df_format[f].apply(lambda x: '${0:,.2f}'.format(x) if not pd.isna(x) else x )\n",
    "        # df_format[f] = df_format[f].apply(lambda x: round(x,1) if not pd.isna(x) else x )\n",
    "    # for fm in format_mil_cols:\n",
    "    #     df_format[fm] = df_format[fm].apply(lambda x: '${0:,.2f}M'.format(x/1e6) if not pd.isna(x) else x )\n",
    "\n",
    "\n",
    "    df_format = df_format.rename(columns={\n",
    "        'date':'Date', 'program_name':'Program', 'num_op': '# OP'\n",
    "        ,'period': 'Period','op_source': 'Source','start_date':'Start','end_date':'End'\n",
    "        ,'cumul_net_dollar_flow_at_program_end':'Net Flows (at End Date)'\n",
    "        ,'cumul_net_dollar_flow':'Net Flows (End + 30)'\n",
    "        ,'cumul_flows_per_op_at_program_end': 'Net Flows per OP (at End Date)'\n",
    "        ,'cumul_flows_per_op_latest': 'Net Flows per OP (End + 30)'\n",
    "        ##\n",
    "        ,'cumul_last_price_net_dollar_flow_at_program_end':'Net Flows @ Current Prices (at End Date)'\n",
    "        ,'cumul_last_price_net_dollar_flow':'Net Flows @ Current Prices (End + 30)'\n",
    "        ,'last_price_net_dollar_flows_per_op_at_program_end': 'Net Flows per OP @ Current Prices (at End Date)'\n",
    "        ,'last_price_net_dollar_flows_per_op_latest': 'Net Flows per OP @ Current Prices (End + 30)'\n",
    "        ,'flows_retention' : 'Net Flows Retained'\n",
    "        ,'last_price_net_dollar_flows_retention' : 'Net Flows Retained @ Current Prices'\n",
    "    })\n",
    "\n",
    "    df_col_list = list(df_format.columns)\n",
    "    df_col_list.remove('include_in_summary')\n",
    "\n",
    "    format_mil_cols_clean = [x for x in df_col_list\n",
    "                             if ('Flows' in x) & ('Retained' not in x)]\n",
    "    # print(format_mil_cols_clean)\n",
    "    format_pct_cols_clean = [x for x in df_col_list\n",
    "                             if 'Retained' in x]\n",
    "\n",
    "    format_op_cols_clean = ['# OP']\n",
    "    # [\n",
    "    #     '# OP','Net Flows (at End Date)',\n",
    "    #     'Net Flows (End + 30)', 'Net Flows @ Current Prices (End + 30)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)'\n",
    "    # ]\n",
    "    df_format = df_format.fillna('')\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "    df_format = df_format.sort_values(by=sort_cols, ascending = sort_order)\n",
    "\n",
    "    # df_format.to_html(\n",
    "    #     prepend + \"img_outputs/app/\" + html_name + \".html\",\n",
    "    #     classes='table table-stripped')\n",
    "    # display(df_format[format_mil_cols_clean])\n",
    "    # fig_tbl = px.table(df_format[df_col_list], sortable=True)\n",
    "    # fig_tbl.show()\n",
    "    \n",
    "    #chatgpt goat?\n",
    "    header = dict(values=df_col_list, fill_color='darkgray', align='center')#, sort_action='native')\n",
    "\n",
    "    # format the numbers in mil_columns and store the result in a list of lists\n",
    "    values = [[pu.format_num(x,'$') if col in format_mil_cols_clean else \n",
    "           pu.format_num(x) if col in format_op_cols_clean else \n",
    "           pu.format_pct(x) if col in format_pct_cols_clean else x \n",
    "           for x in df_format[col]] for col in df_col_list]\n",
    "\n",
    "    cells = dict(values=values, fill_color=['white', 'lightgray'] * (len(df_format)//2+1), align='right')#, line_break=True)\n",
    "\n",
    "    data = [go.Table(header=header, cells=cells)]\n",
    "\n",
    "    layout = go.Layout(title='TVL & Flows Stats')#, width='100%')\n",
    "\n",
    "    fig_tbl = go.Figure(data=data, layout=layout)\n",
    "    # fig_tbl.show()\n",
    "    # pd_html = pu.generate_html(df_format[df_col_list])\n",
    "    # pd_html = pu.DataTable(df_format[df_col_list]).data\n",
    "\n",
    "    # print(type(pd_html))\n",
    "    # open(prepend + \"img_outputs/app/html/\" + html_name + \".html\", \"w\").write(pd_html)\n",
    "\n",
    "\n",
    "    fig_tbl.write_html(prepend+'img_outputs/app/html/'+html_name+'.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee270a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Charts\n",
    "\n",
    "netdf_df = netdf_df[netdf_df['date'] <= pd.to_datetime(\"today\").floor('d')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(netdf_df, x=\"date\", y=\"net_dollar_flow\", color=\"program_name\", \\\n",
    "             title=\"Daily Net Dollar Flow since Program Announcement\",\\\n",
    "            labels={\n",
    "                     \"date\": \"Day\",\n",
    "                     \"net_dollar_flow\": \"Net Dollar Flow (N$F)\"\n",
    "                 }\n",
    "            )\n",
    "fig.update_layout(\n",
    "    legend_title=\"App Name\"\n",
    ")\n",
    "fig.update_layout(yaxis_tickprefix = '$')\n",
    "fig.write_image(prepend + \"img_outputs/svg/daily_ndf.svg\")\n",
    "fig.write_image(prepend + \"img_outputs/png/daily_ndf.png\")\n",
    "fig.write_html(prepend + \"img_outputs/daily_ndf.html\", include_plotlyjs='cdn')\n",
    "\n",
    "# cumul_fig = px.area(netdf_df, x=\"date\", y=\"cumul_net_dollar_flow\", color=\"program_name\", \\\n",
    "#              title=\"Cumulative Dollar Flow since Program Announcement\",\\\n",
    "#                    labels={\n",
    "#                      \"date\": \"Day\",\n",
    "#                      \"cumul_net_dollar_flow\": \"Cumulative Net Dollar Flow (N$F)\"\n",
    "#                  }\n",
    "#             ,areamode='group')\n",
    "# cumul_fig.update_layout(yaxis_tickprefix = '$')\n",
    "# cumul_fig_app.show()\n",
    "\n",
    "\n",
    "cumul_fig = go.Figure()\n",
    "proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    cumul_fig.add_trace(go.Scatter(x=netdf_df[netdf_df['program_name'] == p]['date'] \\\n",
    "                                   , y=netdf_df[netdf_df['program_name'] == p]['cumul_net_dollar_flow'] \\\n",
    "                                    ,name = p\\\n",
    "                                  ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "cumul_fig.update_layout(yaxis_tickprefix = '$')\n",
    "cumul_fig.update_layout(\n",
    "    title=\"Cumulative Net Flows since Program Announcement<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end.</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Flows (USD)\",\n",
    "    legend_title=\"App Name\",\n",
    "#     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "cumul_fig.write_image(prepend + \"img_outputs/svg/cumul_ndf.svg\") #prepend + \n",
    "cumul_fig.write_image(prepend + \"img_outputs/png/cumul_ndf.png\") #prepend + \n",
    "cumul_fig.write_html(prepend + \"img_outputs/cumul_ndf.html\", include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "fig_last = go.Figure()\n",
    "proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    fig_last.add_trace(go.Scatter(x=netdf_df[netdf_df['program_name'] == p]['date'] \\\n",
    "                                   , y=netdf_df[netdf_df['program_name'] == p]['cumul_last_price_net_dollar_flow'] \\\n",
    "                                    ,name = p\\\n",
    "                                  ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "fig_last.update_layout(yaxis_tickprefix = '$')\n",
    "fig_last.update_layout(\n",
    "    title=\"Cumulative Net Flows since Program Announcement (At Most Recent Token Price)<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end.</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Flows (USD) - At Most Recent Price\",\n",
    "    legend_title=\"App Name\",\n",
    "#     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "fig_last.write_image(prepend + \"img_outputs/svg/cumul_ndf_last_price.svg\")\n",
    "fig_last.write_image(prepend + \"img_outputs/png/cumul_ndf_last_price.png\")\n",
    "fig_last.write_html(prepend + \"img_outputs/cumul_ndf_last_price.html\", include_plotlyjs='cdn')\n",
    "# cumul_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program-Specific Charts\n",
    "\n",
    "value_list = ['cumul_net_dollar_flow','cumul_last_price_net_dollar_flow']\n",
    "\n",
    "for val in value_list:\n",
    "  if val == 'cumul_last_price_net_dollar_flow':\n",
    "    postpend = \" - At Last Price\"\n",
    "    folder_path = \"/last_price\"\n",
    "  else:\n",
    "    postpend = \"\"\n",
    "    folder_path = \"\"\n",
    "  proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "  # print(proto_names)\n",
    "  for p in proto_names:\n",
    "      cumul_fig_app = go.Figure()\n",
    "      p_df = netdf_df[netdf_df['program_name'] == p]\n",
    "      # cumul_fig_app = px.area(p_df, x=\"date\", y=\"cumul_net_dollar_flow\", color=\"period\")\n",
    "      \n",
    "      during_df = p_df[p_df['period'] == during_str]\n",
    "      cumul_fig_app.add_trace(go.Scatter(x= during_df['date'] \\\n",
    "                                    , y= during_df[val] \\\n",
    "                                      , name = during_str \\\n",
    "                                    ,fill='tozeroy')) # fill down to xaxis\n",
    "      \n",
    "      post_df = p_df[p_df['period'] == post_str]\n",
    "      cumul_fig_app.add_trace(go.Scatter(x= post_df['date'] \\\n",
    "                                    , y= post_df[val] \\\n",
    "                                      , name = post_str \\\n",
    "                                    ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "      cumul_fig_app.update_layout(yaxis_tickprefix = '$')\n",
    "      cumul_fig_app.update_layout(\n",
    "          title=p + \"<br><sup>Cumulative Net Flows since Program Announcement, Until Program End + 30 Days\" + postpend + \"</sup>\",\n",
    "          xaxis_title=\"Day\",\n",
    "          yaxis_title=\"Cumulative Net Flows (USD)\",\n",
    "          legend_title=\"Period\",\n",
    "      #     color_discrete_map=px.colors.qualitative.G10\n",
    "      )\n",
    "      \n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path):\n",
    "        os.mkdir(prepend + \"img_outputs/app\" + folder_path)\n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path + \"/svg\"):\n",
    "        os.mkdir(prepend + \"img_outputs/app\" + folder_path + \"/svg\")\n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path + \"/png\"):\n",
    "        os.mkdir(prepend + \"img_outputs/app/\" + folder_path + \"/png\")\n",
    "      \n",
    "      p_file = p\n",
    "      p_file = p_file.replace(' ','_')\n",
    "      p_file = p_file.replace(':','')\n",
    "      p_file = p_file.replace('/','-')\n",
    "      cumul_fig_app.write_image(prepend + \"img_outputs/app\" + folder_path + \"/svg/cumul_ndf_\" + p_file + \".svg\") #prepend + \n",
    "      cumul_fig_app.write_image(prepend + \"img_outputs/app\" + folder_path + \"/png/cumul_ndf_\" + p_file + \".png\") #prepend + \n",
    "      cumul_fig_app.write_html(prepend + \"img_outputs/app\" + folder_path + \"/cumul_ndf_\" + p_file + \".html\", include_plotlyjs='cdn')\n",
    "      # cumul_fig_app.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_last.show()\n",
    "print(\"yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python optimism_app_net_flows.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d537a1638226190f579d6fbb68604c1b09ebc740a69df557abedb49ad78e592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
